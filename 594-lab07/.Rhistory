library(tm)
library(twitteR)
library(RColorBrewer)
library(wordcloud)
installr: install.packages("installr")
install.packages("installr")
library(installr)
updateR()
updateR()
install.packages(c("ggplot2", "tm", "wordcloud", "RColorBrewer", "twitteR"))
library(twitteR)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(NLP)
library(twitteR)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(NLP)
directory(C:\Users\jesse\Riles\594-lab02)
directory("C:\Users\jesse\Riles\594-lab02")
directory(C:/Users/jesse/Riles/594-lab02)
setwd(C:/Users/jesse/Riles/594-lab02)
directory(C:Users/jesse/Riles/594-lab02)
setwd(C:Users/jesse/Riles/594-lab02)
# Jessica Embury
# 594 Lab 2
# San Diego County Melanoma Dataset
#### load required libraries
library(ggplot2)
### Read csv data into R dataframe
melanoma_data <- read.csv("Melanoma.csv")
### list all the field names (Variables in the dataset)
names(melanoma_data)
### Subset data by specific column
## subset only the year 2017
data_2017 <- melanoma_data[melanoma_data$Year == 2017,]
### Generate some statistics
### aggregate by region names, make pie chart showing total cases by region
totals_region <- aggregate(data_2017[, c("Total")], by=list(Region = data_2017$Region), FUN=sum, na.rm=TRUE)
colnames(totals_region)[2]  <- "Total"
print(totals_region)
### Make a visual plot ! aes: aesthetic mappings, geom_bar: rectangle bars
totals_bar <- ggplot(totals_region, aes(x=totals_region$Region,y=totals_region$Total)) + geom_bar(stat="identity",aes(fill= factor(totals_region$Region)), width=0.5) +
ggtitle("Melanoma in San Diego by Regions (Total Cases)")+
ylab("Total Cases") +
xlab("Regions of San Deigo") +
theme(legend.position="right") +
theme(axis.text.x = element_text(angle=70, vjust=1, hjust=1))+
list()
## show the plot & save it to file
totals_bar
ggsave("Melanoma_Totals_barchart.png", width = 12, height = 6)
#pie chart of totals - first make barplot then convert to pie
# Barplot
barplot <- ggplot(totals_region, aes(x=" ", y=Total, fill=Region))+
geom_bar(width = 1, stat = "identity")
barplot
#pie formatting
blank_theme <- theme_minimal()+
theme(
axis.title.x = element_blank(),
axis.title.y = element_blank(),
panel.border = element_blank(),
panel.grid=element_blank(),
axis.ticks = element_blank(),
plot.title=element_text(size=14, face="bold")
)
#Convert to pie
totals_pie <- barplot + coord_polar("y", start=0) + theme_minimal() + blank_theme + ggtitle("Melanoma in San Diego by Regions (Total Cases)")
totals_pie
#save
ggsave("Melanoma_Totals_piechart.png", width = 12, height = 6)
install.packages(c("lda", "LDAvis", "plyr", "SnowballC", "stringr"))
# GEOG594 Lab 07: Topic Modeling Using LDA
# Jessica Embury
# Date: 11/12/20
# install R packages
install.packages(c("plyr","stringr","tm","SnowballC","lda","LDAvis"))
library("plyr")
library("stringr")
library("tm")
library("SnowballC")
library("lda")
library("LDAvis")
# set working directory
setwd("C:/Users/jesse/GitProjects/594-Embury/594-lab07/")
# load Twitter data sample
Twitter<-read.csv("TwitterSample.csv")
# create corpus with Tweet texts
Tweets_corpus<-Corpus(VectorSource(Twitter$TWEET_TEXT))
Tweets_corpus[[1]]
# convert to all lowercase with "tolower" function
Tweets_corpus<-tm_map(Tweets_corpus, tolower)
Tweets_corpus[[1]]
# remove punctuation
Tweets_corpus<-tm_map(Tweets_corpus, removePunctuation)
# remove numbers
Tweets_corpus<-tm_map(Tweets_corpus, removeNumbers)
# remove urls
Tweets_corpus<-tm_map(Tweets_corpus, function(x) gsub("http[[:alnum:]]*","", x))
# remove NonASCII characters
Tweets_corpus<-tm_map(Tweets_corpus, function(x) iconv(x, "latin1", "ASCII", sub=""))
# remove stop words
Tweets_corpus<-tm_map(Tweets_corpus, removeWords,stopwords("SMART"))
# remove specific words
Tweets_corpus<-tm_map(Tweets_corpus, removeWords,c("london", "im","ive", "dont", "didnt"))
# format white space
Tweets_corpus<-tm_map(Tweets_corpus, stripWhitespace)
# STEMMING - reduce words to stem by removing suffixes
Tweets_corpus<-tm_map(Tweets_corpus, PlainTextDocument)
Tweets_corpus<-tm_map(Tweets_corpus, stemDocument)
# unlist the text corpus (convert back to character list)
Tweet_Clean<-as.data.frame(unlist(sapply(Tweets_corpus[[1]]$content,'[')), stringsAsFactors=F)
# remove extra white space in text
Tweet_Clean <- lapply(Tweet_Clean[,1], function(x) gsub("^ ", "", x)) #multiple spaces
Tweet_Clean <- lapply(Tweet_Clean, function(x) gsub("^[[:space:]]+", "", x)) #space at the begining
Tweet_Clean <- lapply(Tweet_Clean, function(x) gsub("[[:space:]]+$", "", x)) #space at the end
# bind clean text with Twitter data
Twitter$Tweet_Clean<-Tweet_Clean
# check first 10 Tweets
Twitter[1:10,]
# TOKENIZATION - chop sentences into words (called tokens)
# tokenize on space and output as list
doc.list<-strsplit(unlist(Tweet_Clean), "[[:space:]]+")
# compute table of terms (frequency of each term)
term.table<-table(unlist(doc.list))
term.table<-sort(term.table, decreasing=TRUE)
# remove terms that are stop words or occur < 3 times
term.table<-term.table[term.table>3]
# create a vocabulary with remaining terms
vocab<-names(term.table)
# format documents as required by lda package
get.terms<-function(x){
index<-match(x, vocab)
index<-index[!is.na(index)]
rbind(as.integer(index-1), as.integer(rep(1, length(index))))
}
documents<-lapply(doc.list, get.terms)
# compute statistics related to dataset
D<-length(documents) # number of documents
W<-length(vocan) # number of terms in the vocab
doc.length <- sapply(documents, function(x) sum(x[2, ])) # number of tokens per document
N <- sum(doc.length) # total number of tokens in the data
term.frequency <- as.integer(term.table) # frequencies of terms in the corpus
### fit LDA model
# parameters
K <- 20
G <- 1000
alpha <- 0.1
eta <- 0.1
t1 <- print(Sys.time())
lda_fit <- lda.collapsed.gibbs.sampler (documents = documents, K = K, vocab = vocab, num.iterations = G, alpha = alpha, eta = eta)
t2 <- print(Sys.time())
t2-t1
# show top 20 words of each topic
top_words<-top.topic.words(lda_fit$topics,20,by.score=TRUE)
library("plyr")
library("stringr")
library("tm")
library("SnowballC")
library("lda")
library("LDAvis")
# set working directory
setwd("C:/Users/jesse/GitProjects/594-Embury/594-lab07")
# load Twitter data sample
Twitter<-read.csv("TwitterSample.csv")
# create corpus with Tweet texts
Tweets_corpus<-Corpus(VectorSource(Twitter$TWEET_TEXT))
Tweets_corpus[[1]]
# convert to all lowercase with "tolower" function
Tweets_corpus<-tm_map(Tweets_corpus, tolower)
Tweets_corpus[[1]]
# remove punctuation
Tweets_corpus<-tm_map(Tweets_corpus, removePunctuation)
# remove numbers
Tweets_corpus<-tm_map(Tweets_corpus, removeNumbers)
# remove urls
Tweets_corpus<-tm_map(Tweets_corpus, function(x) gsub("http[[:alnum:]]*","", x))
# remove NonASCII characters
Tweets_corpus<-tm_map(Tweets_corpus, function(x) iconv(x, "latin1", "ASCII", sub=""))
# remove stop words
Tweets_corpus<-tm_map(Tweets_corpus, removeWords,stopwords("SMART"))
# remove specific words
Tweets_corpus<-tm_map(Tweets_corpus, removeWords,c("london", "im","ive", "dont", "didnt"))
# format white space
Tweets_corpus<-tm_map(Tweets_corpus, stripWhitespace)
# STEMMING - reduce words to stem by removing suffixes
Tweets_corpus<-tm_map(Tweets_corpus, PlainTextDocument)
Tweets_corpus<-tm_map(Tweets_corpus, stemDocument)
# unlist the text corpus (convert back to character list)
Tweet_Clean<-as.data.frame(unlist(sapply(Tweets_corpus[[1]]$content,'[')), stringsAsFactors=F)
# remove extra white space in text
Tweet_Clean <- lapply(Tweet_Clean[,1], function(x) gsub("^ ", "", x)) #multiple spaces
Tweet_Clean <- lapply(Tweet_Clean, function(x) gsub("^[[:space:]]+", "", x)) #space at the begining
Tweet_Clean <- lapply(Tweet_Clean, function(x) gsub("[[:space:]]+$", "", x)) #space at the end
# bind clean text with Twitter data
Twitter$Tweet_Clean<-Tweet_Clean
# check first 10 Tweets
Twitter[1:10,]
# TOKENIZATION - chop sentences into words (called tokens)
# tokenize on space and output as list
doc.list<-strsplit(unlist(Tweet_Clean), "[[:space:]]+")
# compute table of terms (frequency of each term)
term.table<-table(unlist(doc.list))
term.table<-sort(term.table, decreasing=TRUE)
# remove terms that are stop words or occur < 3 times
term.table<-term.table[term.table>3]
# create a vocabulary with remaining terms
vocab<-names(term.table)
# format documents as required by lda package
get.terms<-function(x){
index<-match(x, vocab)
index<-index[!is.na(index)]
rbind(as.integer(index-1), as.integer(rep(1, length(index))))
}
documents<-lapply(doc.list, get.terms)
# compute statistics related to dataset
D<-length(documents) # number of documents
W<-length(vocan) # number of terms in the vocab
doc.length <- sapply(documents, function(x) sum(x[2, ])) # number of tokens per document
N <- sum(doc.length) # total number of tokens in the data
term.frequency <- as.integer(term.table) # frequencies of terms in the corpus
### fit LDA model
# parameters
K <- 20
G <- 1000
alpha <- 0.1
eta <- 0.1
t1 <- print(Sys.time())
lda_fit <- lda.collapsed.gibbs.sampler (documents = documents, K = K, vocab = vocab, num.iterations = G, alpha = alpha, eta = eta)
t2 <- print(Sys.time())
t2-t1
# show top 20 words of each topic
top_words<-top.topic.words(lda_fit$topics,20,by.score=TRUE)
# set working directory
setwd("C:/Users/jesse/GitProjects/594-Embury/594-lab07")
# set working directory
setwd("C:\Users\jesse\GitProjects\594-Embury\594-lab07")
# set working directory
setwd("C:/Users/jesse/GitProjects/594-Embury/594-lab07//")
# set working directory
setwd("C:/Users/jesse/GitProjects/594-Embury/594-lab07")
# set working directory
setwd("C:/Users/jesse/GitProjects/GEOG594-Embury/594-lab07")
# load Twitter data sample
Twitter<-read.csv("TwitterSample.csv")
# create corpus with Tweet texts
Tweets_corpus<-Corpus(VectorSource(Twitter$TWEET_TEXT))
Tweets_corpus[[1]]
# convert to all lowercase with "tolower" function
Tweets_corpus<-tm_map(Tweets_corpus, tolower)
Tweets_corpus[[1]]
# remove punctuation
Tweets_corpus<-tm_map(Tweets_corpus, removePunctuation)
# remove numbers
Tweets_corpus<-tm_map(Tweets_corpus, removeNumbers)
# remove urls
Tweets_corpus<-tm_map(Tweets_corpus, function(x) gsub("http[[:alnum:]]*","", x))
# remove NonASCII characters
Tweets_corpus<-tm_map(Tweets_corpus, function(x) iconv(x, "latin1", "ASCII", sub=""))
# remove stop words
Tweets_corpus<-tm_map(Tweets_corpus, removeWords,stopwords("SMART"))
# remove specific words
Tweets_corpus<-tm_map(Tweets_corpus, removeWords,c("london", "im","ive", "dont", "didnt"))
# format white space
Tweets_corpus<-tm_map(Tweets_corpus, stripWhitespace)
# STEMMING - reduce words to stem by removing suffixes
Tweets_corpus<-tm_map(Tweets_corpus, PlainTextDocument)
Tweets_corpus<-tm_map(Tweets_corpus, stemDocument)
# unlist the text corpus (convert back to character list)
Tweet_Clean<-as.data.frame(unlist(sapply(Tweets_corpus[[1]]$content,'[')), stringsAsFactors=F)
# remove extra white space in text
Tweet_Clean <- lapply(Tweet_Clean[,1], function(x) gsub("^ ", "", x)) #multiple spaces
Tweet_Clean <- lapply(Tweet_Clean, function(x) gsub("^[[:space:]]+", "", x)) #space at the begining
Tweet_Clean <- lapply(Tweet_Clean, function(x) gsub("[[:space:]]+$", "", x)) #space at the end
# bind clean text with Twitter data
Twitter$Tweet_Clean<-Tweet_Clean
# check first 10 Tweets
Twitter[1:10,]
# TOKENIZATION - chop sentences into words (called tokens)
# tokenize on space and output as list
doc.list<-strsplit(unlist(Tweet_Clean), "[[:space:]]+")
# compute table of terms (frequency of each term)
term.table<-table(unlist(doc.list))
term.table<-sort(term.table, decreasing=TRUE)
# remove terms that are stop words or occur < 3 times
term.table<-term.table[term.table>3]
# create a vocabulary with remaining terms
vocab<-names(term.table)
# format documents as required by lda package
get.terms<-function(x){
index<-match(x, vocab)
index<-index[!is.na(index)]
rbind(as.integer(index-1), as.integer(rep(1, length(index))))
}
documents<-lapply(doc.list, get.terms)
# compute statistics related to dataset
D<-length(documents) # number of documents
W<-length(vocan) # number of terms in the vocab
doc.length <- sapply(documents, function(x) sum(x[2, ])) # number of tokens per document
N <- sum(doc.length) # total number of tokens in the data
term.frequency <- as.integer(term.table) # frequencies of terms in the corpus
### fit LDA model
# parameters
K <- 20
G <- 1000
alpha <- 0.1
eta <- 0.1
t1 <- print(Sys.time())
lda_fit <- lda.collapsed.gibbs.sampler (documents = documents, K = K, vocab = vocab, num.iterations = G, alpha = alpha, eta = eta)
t2 <- print(Sys.time())
t2-t1
# show top 20 words of each topic
top_words<-top.topic.words(lda_fit$topics,20,by.score=TRUE)
# estimate document-topic distributions for visualizations
theta <- t(apply(lda_fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(lda_fit$topics) + eta, 2, function(x) x/sum(x)))
# save term frequency, ϕ, θ, and vocab, in a list as the data object “Tweet_Topics”.
Tweet_Topics <- list(phi = phi, theta = theta, doc.length = doc.length, vocab = vocab, term.frequency = term.frequency)
# create json object for viz
Tweet_Topics_json <- with(Tweet_Topics,createJSON(phi, theta, doc.length, vocab, term.frequency))
# display interactive visual using serVis
serVis(Tweet_Topics_json)
# assign Tweet to the topic that occurs most frequently within the Tweet
doc_topic <- apply(lda_fit$document_sums, 2, function(x) which(x == max(x))[1])
Twitter$topic<-doc_topic
# save file for visualization in ArcGIS Online
TwitterDf <- data.frame(lapply(Twitter, as.character), stringsAsFactors=FALSE)
write.csv(TwitterDf,"TwitterWithTopic.csv")
